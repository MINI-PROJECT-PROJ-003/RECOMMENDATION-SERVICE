# -*- coding: utf-8 -*-
"""Copy of cosine.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qz0Y-2BCKMwXLxHEih87kDssxAZ9AFPe
"""

import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
!pip install surprise
from surprise import Dataset 
from surprise import Reader
from surprise.model_selection import train_test_split
from surprise import accuracy
from surprise.model_selection import GridSearchCV,RandomizedSearchCV
from sklearn.metrics import confusion_matrix, precision_score, recall_score,classification_report

"""#Loading Data

#Importing Libraries
"""

df = pd.read_csv("ratings1.csv")
df.head()

"""# Data preprocessing"""

df.isna().sum() #checking for missing value column wise

dup_bool = df.duplicated(['userId','movieId','rating'])
print("Number of duplicate records:",sum(dup_bool))              #Checking for duplicate records

"""#Basic data exploration"""

print("Total no of ratings :",df.shape[0])
print("No. of unique users:", df["userId"].nunique())
print("No. of unique movies:", df["movieId"].nunique())        #Total number of users,movies and ratings

fig, ax = plt.subplots(figsize=(12,8))
ax.set_title('Ratings distribution', fontsize=15)
sns.countplot(df['rating'])
ax.set_xlabel("ratings in interval")
ax.set_ylabel("Total number of ratings")                       #Distribution of ratings

ratings_per_user = df.groupby(by='userId')['rating'].count()#.sort_values(ascending=False)
ratings_per_user.describe()                       #rating per user

ratings_per_movie = df.groupby(by='movieId')['rating'].count()
ratings_per_movie.describe()                #rating per movie

"""#Loading as Surprise dataframe and train-test split"""

reader = Reader()
ratings = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)

"""Splitting into train and test set"""

train_ratings, test_ratings = train_test_split(ratings, test_size=.20, random_state = 42)
print("Size of trainset: ", train_ratings.n_ratings)
print("Size of testset: ", len(test_ratings))

"""#Using KNNBasic"""

from surprise import KNNBasic
knn_model = KNNBasic(random_state = 42,verbose = False)
knn_model.fit(train_ratings)

#Evaluating the results
train_predictions = knn_model.test(train_ratings.build_testset())
test_predictions = knn_model.test(test_ratings)
print("RMSE on training data : ", accuracy.rmse(train_predictions, verbose = False))
print("RMSE on test data: ", accuracy.rmse(test_predictions, verbose = False))

param_grid = {'k': list(range(10,45,5)),
             'min_k' : list(range(5,11))}
gs = GridSearchCV(KNNBasic, param_grid, measures=['rmse'], return_train_measures = True, cv = 5)
gs.fit(ratings)
gs.best_params['rmse']   #Hyper-parameter tuning to find optimal value of k and min_k

tuned_knn_model = KNNBasic(k = 15, min_k= 5,random_state = 42, verbose = False)
tuned_knn_model.fit(train_ratings)
train_predictions = tuned_knn_model.test(train_ratings.build_testset())
test_predictions = tuned_knn_model.test(test_ratings)
print("RMSE on training data : ", accuracy.rmse(train_predictions, verbose = False))
print("RMSE on test data: ", accuracy.rmse(test_predictions, verbose = False))

"""#Item-item similarity based

"""

#Fitting model
knn_model_item_based = KNNBasic(user_based = False, random_state = 42)
knn_model_item_based.fit(train_ratings)

#Evaluating the results
train_predictions = knn_model_item_based.test(train_ratings.build_testset())
test_predictions = knn_model_item_based.test(test_ratings)
print("RMSE on training data : ", accuracy.rmse(train_predictions, verbose = False))
print("RMSE on test data: ", accuracy.rmse(test_predictions, verbose = False))

